from operator import matmul
import random
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np 
#from d2l import tensorflow as d2l

# Generating synthetic dataset with Linear Model : y = Xw+b+noise

def synthetic_data(w, b, num_examples):  
    X = tf.zeros((num_examples, w.shape[0]))
    X += tf.random.normal(shape=X.shape)
    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b
    y += tf.random.normal(shape=y.shape, stddev=0.01)
    y = tf.reshape(y, (-1, 1))
    return X, y

true_w = tf.constant([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000) #storing value of X and y respectively 
print('features:', features[0],'\nlabel:', labels[0])
# Scatter plot can be generated of said normally distributed random data with noise 

# function to read the data set 

def read_data(features,labels,batch_size):
    num_examples = len(features) # compute length of numerical examples
    indices = list(range(num_examples))
    random.shuffle(indices) # shuffle all indices randomly so that data is read randomly
    for i in range(0,batch_size,num_examples):
        # have to read data in mini batches therefore :
        j = tf.constant(indices[i: min(i + batch_size, num_examples)]) # tensor j holds the size of mini batches
        yield(tf.gather(features,j),tf.gather(labels,j)) # yield mini batches with features and labels as tuples
    return 
# reading data in batch sizes of 10 and printing
batch_size = 10
for X, y in read_data(features, labels,batch_size):
    print(X, '\n', y)
    break

# Initialising model Parameters weight and bias : 
w = tf.Variable(tf.random.normal(shape=(2, 1), mean=0, stddev=0.01),trainable=True)
b = tf.Variable(tf.zeros(1), trainable=True)
# we will use auto diff to update our model params to compute gradient of the model 

#Defining the model
def linreg (X,w,b):
    return matmul(X,w)+b

# Defining the loss function here we use : MSE 
def squared_loss(y_hat,y):
    return (y_hat - tf.reshape(y, y_hat.shape)) ** 2 / 2
    # MSE : (y_predicted - y_actual)**2/2

# Defining an Optimisation Function : we are using MiniBatch SGD
def sgd(params,grads,lr,batch_size):
    for params,grads in zip(params,grads):
        params.assign_sub(lr*grads/batch_size)
    
# Training the model by iteration 
lr = 0.03
num_epochs = 10
net = linreg
loss = squared_loss

# training loop for the lin reg
for epoch in range(num_epochs):
    for X, y in read_data(features, labels,batch_size):
        with tf.GradientTape() as g:
            l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`
        # Compute gradient on l with respect to [`w`, `b`]
        dw, db = g.gradient(l, [w, b])
        # Update parameters using their gradient
        sgd([w, b], [dw, db], lr, batch_size) # pass parameters through sgd optimiser 
    train_l = loss(net(features, w, b), labels) # store value generated by loss function into train 
    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}') # print the error in b and w 

print(f'error in estimating w: {true_w - tf.reshape(w, true_w.shape)}')
print(f'error in estimating b: {true_b - b}')